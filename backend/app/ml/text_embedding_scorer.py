"""
================================================================================
IMPORTANT NOTE - SCORING METHODOLOGY
================================================================================

This code is NOT used dynamically in our main production system. This is one of 
several methods we employ specifically for scoring multiple-choice options in our 
scenarios during the development and validation phase.

Our complete scoring methodology includes:
1. Text Embedding Similarity (this script) - Semantic similarity to ideal answers
2. LLM-based Evaluation - Multiple LLMs (GPT-4, Claude, etc.) provide qualitative scores
3. Expert Human Review - Domain experts validate the scores
4. Statistical Analysis - Inter-rater reliability and consensus scoring

The final scores for each option are determined by combining all these methods,
not solely relying on embedding similarity. This multi-faceted approach ensures
more robust and accurate evaluation of analytical thinking patterns.

Use cases for this script:
- Initial automated scoring of new scenarios
- Consistency checking across similar questions
- Baseline establishment for option quality
- Training data generation for fine-tuned models

This is a SUPPLEMENTARY TOOL in our comprehensive evaluation pipeline.
================================================================================
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import openai
from typing import Dict, List, Tuple
import json

class ScenarioEvaluator:
    """
    Text Embedding-based Multiple Choice Question Evaluator
    
    This class evaluates multiple choice answers by comparing them to an ideal reference answer
    using semantic similarity through text embeddings. The system is designed to automatically
    score answers based on how closely they align with analytical thinking patterns.
    """
    
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Initialize the evaluator with a sentence transformer model.
        
        Args:
            model_name (str): Name of the pre-trained sentence transformer model to use.
                            Default is a multilingual model that supports Turkish.
        
        Available model options:
        - 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': Fast, lightweight, supports 50+ languages
        - 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2': More accurate, larger model
        - 'emrecan/bert-base-turkish-cased-mean-nli-stsb-tr': Turkish-specific BERT model
        - 'sentence-transformers/all-MiniLM-L6-v2': English-only, very fast
        
        The model converts text into dense vector representations (embeddings) that capture
        semantic meaning, allowing mathematical comparison of text similarity.
        """
        self.model = SentenceTransformer(model_name)
        self.openai_api_key = None  # Optional: Set if you want to use GPT for reference generation
        
    def generate_reference_text(self, scenario: str, use_gpt: bool = False, api_key: str = None) -> str:
        """
        Generate an ideal reference answer that represents high analytical thinking.
        
        This function creates the "gold standard" answer that all options will be compared against.
        It can either use a pre-written analytical response or generate one using GPT-4.
        
        Args:
            scenario (str): The scenario/question text to generate a reference answer for
            use_gpt (bool): Whether to use GPT-4 API to generate the reference (requires API key)
            api_key (str): OpenAI API key for GPT-4 access
        
        Returns:
            str: A comprehensive analytical reference answer that embodies:
                 - Data-driven decision making
                 - Systematic approach to problem solving
                 - Segmentation and targeting strategies
                 - Testing and optimization mindset
                 - Resource efficiency considerations
        """
        if use_gpt and api_key:
            # Use GPT-4 to generate a high-quality analytical reference answer
            openai.api_key = api_key
            
            prompt = f"""You are a highly analytical strategy expert with exceptional data-driven decision-making skills. 
            
            Scenario: {scenario}
            
            How would you approach this scenario? Create a detailed, analytical strategy.
            Focus on: data analysis, segmentation, pilot testing, measurement, and optimization.
            Respond in the same language as the scenario."""
            
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # Lower temperature for more consistent, analytical responses
                max_tokens=500
            )
            return response.choices[0].message['content']
        else:
            # Pre-written ideal analytical reference answer (in Turkish for this example)
            # This represents the thinking pattern of someone with high analytical skills
            return """
            Analitik yaklaÅŸÄ±mla ÅŸu stratejiyi uygularÄ±m:
            
            1. Veri Analizi ve Segmentasyon: Ã–ncelikle mevcut verileri detaylÄ± analiz ederim. 
            Her yaÅŸ grubunun Ã¶zelliklerini, dijital alÄ±ÅŸkanlÄ±klarÄ±nÄ± ve Ã§evre konularÄ±na yaklaÅŸÄ±mlarÄ±nÄ± incelerim.
            
            2. Pilot Test YaklaÅŸÄ±mÄ±: Ä°lk 3 gÃ¼nde her segment iÃ§in kÃ¼Ã§Ã¼k Ã¶lÃ§ekli pilot testler yaparÄ±m. 
            FarklÄ± iÃ§erik formatlarÄ± ve mesajlaÅŸma stratejilerini test ederek gerÃ§ek zamanlÄ± veri toplarÄ±m.
            
            3. Kanal Optimizasyonu: 
            - 15-18 yaÅŸ iÃ§in kÄ±sa, etkileyici video iÃ§erikler (TikTok, Instagram Reels)
            - 25-35 yaÅŸ iÃ§in veri odaklÄ±, bilgilendirici infografikler (LinkedIn, Twitter)  
            - 35+ yaÅŸ iÃ§in geleneksel medya ve WhatsApp gruplarÄ± kombinasyonu
            
            4. BÃ¼tÃ§e DaÄŸÄ±lÄ±mÄ±: Pilot test sonuÃ§larÄ±na gÃ¶re dinamik bÃ¼tÃ§e ayarlamasÄ± yaparÄ±m. 
            En yÃ¼ksek ROI saÄŸlayan segment ve kanallara kaynak aktarÄ±mÄ± yaparÄ±m.
            
            5. A/B Testing: SÃ¼rekli A/B testlerle mesajlarÄ± optimize ederim. 
            Her gÃ¼n performans metriklerini analiz eder, stratejiyi gÃ¼ncellerim.
            
            6. Ã‡apraz Segment Sinerji: Segmentler arasÄ± viral potansiyeli olan iÃ§erikler oluÅŸtururum.
            Ã–rneÄŸin genÃ§lerin Ã¼rettiÄŸi iÃ§eriklerin bÃ¼yÃ¼klere ulaÅŸmasÄ± iÃ§in kÃ¶prÃ¼ler kurarÄ±m.
            
            7. Ã–lÃ§Ã¼mleme: Engagement rate, conversion rate, reach, viral coefficient gibi KPI'larÄ± 
            gÃ¼nlÃ¼k takip eder, veri odaklÄ± kararlarla kampanyayÄ± optimize ederim.
            """
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        Convert text strings into numerical embedding vectors.
        
        This is the core of the semantic similarity system. Each text is transformed into
        a high-dimensional vector (typically 384 or 768 dimensions) where similar meanings
        result in vectors that are close together in vector space.
        
        Args:
            texts (List[str]): List of text strings to convert to embeddings
        
        Returns:
            np.ndarray: Matrix where each row is an embedding vector for the corresponding text
                       Shape: (num_texts, embedding_dimension)
        
        Technical details:
        - The model uses transformer architecture to understand context
        - Each word influences the representation of other words (attention mechanism)
        - The final embedding captures the overall semantic meaning of the entire text
        """
        embeddings = self.model.encode(texts)
        return embeddings
    
    def calculate_similarity_scores(self, reference_embedding: np.ndarray, 
                                   option_embeddings: np.ndarray) -> List[float]:
        """
        Calculate cosine similarity between reference and each option embedding.
        
        Cosine similarity measures the cosine of the angle between two vectors:
        - 1.0 = identical direction (maximum similarity)
        - 0.0 = orthogonal (no similarity)
        - -1.0 = opposite direction (maximum dissimilarity)
        
        For text embeddings, scores typically range from 0.2 to 0.9
        
        Args:
            reference_embedding (np.ndarray): The ideal answer's embedding vector
            option_embeddings (np.ndarray): Matrix of option embeddings to compare
        
        Returns:
            List[float]: Similarity scores for each option (0 to 1 scale)
        
        Mathematical formula:
        cosine_similarity(A, B) = dot_product(A, B) / (norm(A) * norm(B))
        """
        similarities = cosine_similarity([reference_embedding], option_embeddings)[0]
        return similarities.tolist()
    
    def evaluate_scenario(self, scenario: str, options: Dict[str, str], 
                         reference_text: str = None, use_gpt: bool = False, 
                         api_key: str = None) -> Dict:
        """
        Main evaluation pipeline - orchestrates the entire scoring process.
        
        This method:
        1. Generates or uses a reference answer
        2. Converts all texts to embeddings
        3. Calculates similarity scores
        4. Normalizes scores to 0-100 scale
        5. Ranks options by score
        
        Args:
            scenario (str): The question/scenario text
            options (Dict[str, str]): Dictionary mapping option labels (A, B, C, etc.) to answer texts
            reference_text (str, optional): Pre-defined reference answer (if not provided, will generate)
            use_gpt (bool): Whether to use GPT-4 for reference generation
            api_key (str, optional): OpenAI API key
        
        Returns:
            Dict: Comprehensive results including:
                  - 'reference_text': The ideal answer used for comparison
                  - 'scores': Detailed scoring for each option
                  - 'rankings': Options sorted by score (best to worst)
        
        Score normalization explanation:
        - Raw cosine similarities might range from 0.3 to 0.7
        - We normalize to 0-100 scale for better interpretability
        - The best option gets 100, worst gets 0, others scaled proportionally
        """
        # Step 1: Generate or use provided reference text
        if reference_text is None:
            reference_text = self.generate_reference_text(scenario, use_gpt, api_key)
        
        print("ğŸ“Š Reference text generated - representing ideal analytical thinking")
        print("-" * 50)
        
        # Step 2: Create embeddings for reference and all options
        # We process all texts in one batch for efficiency
        all_texts = [reference_text] + list(options.values())
        embeddings = self.create_embeddings(all_texts)
        
        # Separate reference embedding from option embeddings
        reference_embedding = embeddings[0]
        option_embeddings = embeddings[1:]
        
        # Step 3: Calculate raw similarity scores
        similarity_scores = self.calculate_similarity_scores(reference_embedding, option_embeddings)
        
        # Step 4: Prepare results dictionary
        results = {
            'reference_text': reference_text,
            'scores': {},
            'rankings': []
        }
        
        # Step 5: Normalize scores to 0-100 scale for better interpretability
        # This makes it easier to understand relative differences
        min_score = min(similarity_scores)
        max_score = max(similarity_scores)
        
        for option_key, score in zip(options.keys(), similarity_scores):
            # Linear normalization: (value - min) / (max - min) * 100
            # If all scores are identical, set to 50
            normalized_score = ((score - min_score) / (max_score - min_score)) * 100 if max_score > min_score else 50
            
            results['scores'][option_key] = {
                'raw_score': float(score),  # Original cosine similarity (0-1)
                'normalized_score': float(normalized_score),  # Scaled to 0-100
                'text': options[option_key][:100] + '...'  # First 100 chars for preview
            }
        
        # Step 6: Create ranking (sorted by normalized score, descending)
        results['rankings'] = sorted(
            results['scores'].items(), 
            key=lambda x: x[1]['normalized_score'], 
            reverse=True
        )
        
        return results
    
    def print_results(self, results: Dict):
        """
        Display evaluation results in a visually appealing format.
        
        Creates a console output with:
        - Ranking table with visual progress bars
        - Detailed score breakdown
        - Easy-to-read formatting
        
        Args:
            results (Dict): Results dictionary from evaluate_scenario()
        
        Visual elements:
        - Progress bars: â–ˆ for filled, â–‘ for empty
        - Percentage scores for quick interpretation
        - Ranked list to identify best/worst options immediately
        """
        print("\n" + "="*60)
        print("ğŸ“ˆ EVALUATION RESULTS")
        print("="*60)
        
        print("\nğŸ† RANKING (Best to Worst):")
        print("-"*40)
        
        # Display each option with visual progress bar
        for rank, (option, scores) in enumerate(results['rankings'], 1):
            score = scores['normalized_score']
            
            # Create visual progress bar (20 characters wide)
            bar_length = int(score / 5)  # Each block represents 5%
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            
            # Print ranking with visual elements
            print(f"{rank}. Option {option}: {bar} {score:.1f}%")
            print(f"   Cosine Similarity: {scores['raw_score']:.4f}")
            print()
        
        print("\nğŸ“ DETAILED SCORES:")
        print("-"*40)
        
        # Show detailed breakdown for each option
        for option, scores in results['scores'].items():
            print(f"Option {option}:")
            print(f"  - Raw Score (0-1 scale): {scores['raw_score']:.4f}")
            print(f"  - Normalized Score: {scores['normalized_score']:.2f}/100")
            print()

# ================================================================================
# USAGE EXAMPLE - Demonstrates how to use the ScenarioEvaluator
# ================================================================================

if __name__ == "__main__":
    
    # Define the scenario/question that needs to be evaluated
    # This is a complex decision-making scenario requiring analytical thinking
    scenario = """
    Bir sivil toplum kuruluÅŸu, farklÄ± yaÅŸ gruplarÄ±na Ã§evre bilinci kazandÄ±rmayÄ± amaÃ§layan bir kampanya baÅŸlatmak istiyor. 
    Sizden, kampanyanÄ±n etki dÃ¼zeyini artÄ±rmak iÃ§in bir strateji geliÅŸtirmeniz isteniyor. 
    Elinizde sÄ±nÄ±rlÄ± bir bÃ¼tÃ§e ve yalnÄ±zca iki hafta sÃ¼reniz var. Kurum size bazÄ± verileri sunuyor:
    * 15-18 yaÅŸ arasÄ± bireyler sosyal medya Ã¼zerinden kolay eriÅŸiliyor ancak ilgileri daÄŸÄ±nÄ±k.
    * 25-35 yaÅŸ grubu Ã§evreye duyarlÄ± ancak iÃ§eriklerin bilgi yoÄŸunluÄŸu onlarÄ±n ilgisini Ã§ekiyor.
    * 35 yaÅŸ Ã¼stÃ¼ bireyler, Ã§evre konularÄ±na daha duyarlÄ± ancak dijital platformlara eriÅŸimleri sÄ±nÄ±rlÄ±.
    """
    
    # Define the multiple choice options
    # Each represents a different approach to the problem
    options = {
        'A': """YaÅŸ gruplarÄ±na gÃ¶re segmentasyon yapar, her grup iÃ§in farklÄ± iletiÅŸim kanallarÄ± ve iÃ§erikler belirlerim. 
                Ã–nceki kampanya verilerini analiz ederek en etkili kombinasyonlarÄ± belirlerim ve bunlarÄ± uygularÄ±m.""",
        # Expected: HIGH SCORE - Shows segmentation and data analysis
        
        'B': """Ã–nce 25-35 yaÅŸ grubuna odaklanÄ±rÄ±m Ã§Ã¼nkÃ¼ en dengeli grup bu. 
                TÃ¼m kaynaklarÄ± bu hedef kitleye ayÄ±rÄ±r, maksimum etkiyi bu gruptan almayÄ± hedeflerim.""",
        # Expected: MEDIUM SCORE - Focused but not comprehensive
        
        'C': """Elde edilen verilerin hepsini doÄŸrudan Ã¼st yÃ¶netime sunarÄ±m. 
                OnlarÄ±n deneyimiyle karar vermelerini isterim, ben uygulama kÄ±smÄ±nda yer alÄ±rÄ±m.""",
        # Expected: LOW SCORE - Avoids responsibility and analytical thinking
        
        'D': """TÃ¼m gruplara tek tip iÃ§erik hazÄ±rlarÄ±m. BÃ¶ylece zaman ve kaynak kaybÄ± yaÅŸamam. 
                YaygÄ±n ve genel geÃ§er mesajlarla kampanyayÄ± yayarÄ±m.""",
        # Expected: LOW SCORE - Ignores segmentation and data
        
        'E': """Ã–nce kÃ¼Ã§Ã¼k bir pilot grup Ã¼zerinde farklÄ± stratejiler denerim. 
                Elde ettiÄŸim verilere gÃ¶re hangi grubun hangi iÃ§erikle daha Ã§ok etkileÅŸime geÃ§tiÄŸini Ã¶lÃ§er 
                ve uygulamayÄ± bu verilere gÃ¶re Ã¶lÃ§eklendiririm."""
        # Expected: HIGH SCORE - Data-driven, testing-oriented approach
    }
    
    # Initialize the evaluator with default multilingual model
    evaluator = ScenarioEvaluator()
    
    # Run the evaluation
    # Set use_gpt=True and provide api_key to use GPT-4 for reference generation
    results = evaluator.evaluate_scenario(
        scenario=scenario,
        options=options,
        use_gpt=False  # Set to True and add api_key="your-key" to use GPT-4
    )
    
    # Display results in console
    evaluator.print_results(results)
    
    # Optional: Save results to JSON file for further analysis
    # This creates a structured data file that can be used for:
    # - Tracking evaluation history
    # - Statistical analysis
    # - Integration with other systems
    with open('evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        print("\nğŸ’¾ Results saved to 'evaluation_results.json'")
    
    # Example of accessing specific scores programmatically
    print("\n" + "="*60)
    print("ğŸ“Š PROGRAMMATIC ACCESS EXAMPLE:")
    print("-"*40)
    best_option = results['rankings'][0][0]
    best_score = results['rankings'][0][1]['normalized_score']
    print(f"Best option: {best_option} with score {best_score:.2f}%")
    
    worst_option = results['rankings'][-1][0]
    worst_score = results['rankings'][-1][1]['normalized_score']
    print(f"Worst option: {worst_option} with score {worst_score:.2f}%")
    
    # ================================================================================
    # ADDITIONAL NOTES ON SCORING METHODOLOGY
    # ================================================================================
    # Remember: These embedding-based scores are just ONE component of our evaluation.
    # In production, we combine these scores with:
    # 
    # 1. LLM Evaluations (30-40% weight):
    #    - GPT-4 analytical assessment
    #    - Claude reasoning evaluation  
    #    - Gemini logical consistency check
    #
    # 2. Human Expert Scores (30-40% weight):
    #    - Domain experts rate each option
    #    - Consensus meetings for edge cases
    #
    # 3. Embedding Similarity (20-30% weight):
    #    - This script's output
    #    - Cross-validated with multiple models
    #
    # Final Score = Weighted average of all methods
    # This ensures robust, multi-perspective evaluation of analytical thinking
    # ================================================================================